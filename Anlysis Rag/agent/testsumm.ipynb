{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_16396\\825852704.py:25: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_16396\\825852704.py:49: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2:3b\")\n",
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI don't see an uploaded document. This conversation has just started, and I'm ready to assist you with any questions or tasks you'd like to discuss. Could you please provide the document or information you'd like me to summarize?\n",
      "\n",
      "If you'd like, we can also explore specific topics or areas of research that might be relevant to your needs. Just let me know what's on your mind!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "📌 **الإجابة النهائية:**\n",
      "{'input': 'Summarize the uploaded document and find relevant Wikipedia and Arxiv references.', 'output': \"I don't see an uploaded document. This conversation has just started, and I'm ready to assist you with any questions or tasks you'd like to discuss. Could you please provide the document or information you'd like me to summarize?\\n\\nIf you'd like, we can also explore specific topics or areas of research that might be relevant to your needs. Just let me know what's on your mind!\"}\n"
     ]
    }
   ],
   "source": [
    "# استيراد المكتبات الأساسية\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "# 🔹 **1. تحميل البيانات**\n",
    "# يمكنك استبدال \"your_file.txt\" باسم الملف الذي تريد تلخيصه\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "# 🔹 **2. تقسيم النص إلى أجزاء مناسبة**\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 🔹 **3. تحويل النصوص إلى تمثيلات رقمية باستخدام Embeddings**\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# 🔹 **4. تخزين البيانات في قاعدة بيانات متجهية (FAISS)**\n",
    "vector_db = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# 🔹 **5. إنشاء RAG Retriever**\n",
    "retriever = vector_db.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"document_search\",\n",
    "    \"Search for relevant information from the uploaded document.\"\n",
    ")\n",
    "\n",
    "# 🔹 **6. إعداد مصادر أخرى مثل Wikipedia و Arxiv**\n",
    "wiki_api = WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
    "\n",
    "arxiv_api = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_api)\n",
    "\n",
    "# 🔹 **7. تجهيز Agent لاستخدام RAG والمصادر الخارجية**\n",
    "tools = [retriever_tool, wiki_tool, arxiv_tool]\n",
    "\n",
    "# استخدام LLM من Ollama\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# تحميل الـ Prompt Template من LangChain\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# إنشاء الـ Agent باستخدام RAG والمصادر الخارجية\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# 🔹 **8. تنفيذ الاستعلام باستخدام Agent و RAG**\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": \"Summarize the uploaded document and find relevant Wikipedia and Arxiv references.\"})\n",
    "\n",
    "print(\"📌 **الإجابة النهائية:**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 عدد المستندات المقسمة: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 اختبار استرجاع البيانات من RAG: [Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'), Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'), Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'), Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_search` with `{'query': 'summarize uploaded document'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "11\n",
      "\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'summarize uploaded document'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: PDF\n",
      "Summary: Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems. Based on the PostScript language, ea\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'summarize uploaded document'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mPublished: 2023-06-20\n",
      "Title: QuOTeS: Query-Oriented Technical Summarization\n",
      "Authors: Juan Ramirez-Orta, Eduardo Xamena, Ana Maguitman, Axel J. Soto, Flavia P. Zanoto, Evangelos Milios\n",
      "Summary: Abstrac\u001b[0m\u001b[32;1m\u001b[1;3mBased on the uploaded document, it appears to be a research paper titled \"QuOTeS: Query-Oriented Technical Summarization\". The summary of the document is as follows:\n",
      "\n",
      "The paper proposes a novel approach for technical summarization, which is a task that aims to generate a concise summary of a technical text based on a query. The authors introduce a new model called QuOTeS, which relies on self-attention mechanisms and transformer architecture to compute representations of input sequences.\n",
      "\n",
      "Relevant Wikipedia references:\n",
      "\n",
      "* Transformer: A type of neural network architecture used for natural language processing tasks, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n",
      "* Self-attention: A mechanism used in transformer architectures to compute attention weights between different elements in a sequence, allowing the model to focus on relevant parts of the input.\n",
      "\n",
      "Relevant Arxiv references:\n",
      "\n",
      "* [4] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.\n",
      "* [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.\n",
      "* [27] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n",
      "\n",
      "Note that the document also mentions other relevant research papers and models in the field of natural language processing and machine learning, including those mentioned in the section \"Related Work\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "📌 **الإجابة النهائية:**\n",
      "{'input': 'Summarize the uploaded document and find relevant Wikipedia and Arxiv references.', 'output': 'Based on the uploaded document, it appears to be a research paper titled \"QuOTeS: Query-Oriented Technical Summarization\". The summary of the document is as follows:\\n\\nThe paper proposes a novel approach for technical summarization, which is a task that aims to generate a concise summary of a technical text based on a query. The authors introduce a new model called QuOTeS, which relies on self-attention mechanisms and transformer architecture to compute representations of input sequences.\\n\\nRelevant Wikipedia references:\\n\\n* Transformer: A type of neural network architecture used for natural language processing tasks, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\\n* Self-attention: A mechanism used in transformer architectures to compute attention weights between different elements in a sequence, allowing the model to focus on relevant parts of the input.\\n\\nRelevant Arxiv references:\\n\\n* [4] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.\\n* [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.\\n* [27] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\\n\\nNote that the document also mentions other relevant research papers and models in the field of natural language processing and machine learning, including those mentioned in the section \"Related Work\".'}\n"
     ]
    }
   ],
   "source": [
    "# 🔹 تحديث استيراد مكتبة Ollama\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "# ✅ **1. تحميل الملف النصي**\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# ✅ **2. تقسيم النص إلى أجزاء**\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"🔹 عدد المستندات المقسمة: {len(split_docs)}\")  # التحقق من وجود بيانات\n",
    "\n",
    "# ✅ **3. تحويل النصوص إلى Embeddings وتخزينها**\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "vector_db = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# ✅ **4. إنشاء RAG Retriever**\n",
    "retriever = vector_db.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"document_search\",\n",
    "    \"Search for relevant information from the uploaded document.\"\n",
    ")\n",
    "\n",
    "# ✅ **5. إضافة Wikipedia و Arxiv كمصادر إضافية**\n",
    "wiki_api = WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
    "\n",
    "arxiv_api = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_api)\n",
    "\n",
    "# ✅ **6. تجهيز Agent**\n",
    "tools = [retriever_tool, wiki_tool, arxiv_tool]\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# ✅ **7. اختبار الاسترجاع من قاعدة البيانات قبل تشغيل Agent**\n",
    "retrieval_test = retriever.invoke(\"summarize\")\n",
    "print(\"🔹 اختبار استرجاع البيانات من RAG:\", retrieval_test)\n",
    "\n",
    "# ✅ **8. تنفيذ الاستعلام باستخدام Agent**\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": \"Summarize the uploaded document and find relevant Wikipedia and Arxiv references.\"})\n",
    "\n",
    "print(\"📌 **الإجابة النهائية:**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `data_analysis` with `{'query': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mregion_id: 70 | sales_district: Salem | sales_region: North West\n",
      "region_id: 71 | sales_district: Salem | sales_region: North West\n",
      "region_id: 72 | sales_district: Salem | sales_region: North West\n",
      "region_id: 73 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 74 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 75 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 76 | sales_district: Bremerton | sales_region: North West\n",
      "region_id: 77 | sales_district: Bremerton | sales_region: North West\n",
      "region_id: 78 | sales_district: Bellingham | sales_region: North West\n",
      "region_id: 79 | sales_district: Bellingham | sales_region: North West\n",
      "region_id: 80 | sales_district: Bellingham | sales_region: North West\n",
      "region_id: 81 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 82 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 83 | sales_district: Seattle | sales_region: North West\n",
      "\n",
      "region_id: 82 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 83 | sales_district: Seattle | sales_region: North West\n",
      "region_id: 84 | sales_district: Tacoma | sales_region: North West\n",
      "region_id: 85 | sales_district: Tacoma | sales_region: North West\n",
      "region_id: 86 | sales_district: Tacoma | sales_region: North West\n",
      "region_id: 87 | sales_district: Spokane | sales_region: North West\n",
      "region_id: 88 | sales_district: Walla Walla | sales_region: North West\n",
      "region_id: 89 | sales_district: Yakima | sales_region: North West\n",
      "region_id: 90 | sales_district: Vancouver | sales_region: Canada West\n",
      "region_id: 91 | sales_district: Vancouver | sales_region: Canada West\n",
      "region_id: 92 | sales_district: Vancouver | sales_region: Canada West\n",
      "region_id: 93 | sales_district: Vancouver | sales_region: Canada West\n",
      "region_id: 94 | sales_district: Vancouver | sales_region: Canada West\n",
      "region_id: 95 | sales_district: Vancouver | sales_region: Canada West\n",
      "\n",
      "region_id: 106 | sales_district: Mexico City | sales_region: Mexico Central\n",
      "region_id: 107 | sales_district: Mexico City | sales_region: Mexico Central\n",
      "region_id: 108 | sales_district: Mexico City | sales_region: Mexico Central\n",
      "region_id: 109 | sales_district: Mexico City | sales_region: Mexico Central\n",
      "\n",
      "region_id: 47 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 48 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 49 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 50 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 51 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 52 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 53 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 54 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 55 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 56 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 57 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 58 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 59 | sales_district: Los Angeles | sales_region: South West\n",
      "region_id: 60 | sales_district: Los Angeles | sales_region: South West\u001b[0m\u001b[32;1m\u001b[1;3m**Analysis of the Dataset**\n",
      "\n",
      "The dataset consists of 114 regions with their corresponding sales districts and regions. Here are some key observations:\n",
      "\n",
      "1. **North West Region**: The North West region has 23 regions, all located in Washington state. This suggests a strong focus on this region.\n",
      "2. **South West Region**: The South West region has 21 regions, all located in California. This also indicates a significant presence in this region.\n",
      "3. **Mexico Central Region**: There are three regions in the Mexico Central region, indicating a smaller but still notable presence in this area.\n",
      "4. **Canada West Region**: Five regions are located in the Canada West region, primarily in Vancouver.\n",
      "\n",
      "**Key Insights**\n",
      "\n",
      "* The North West and South West regions have a strong focus on California and Washington state, respectively.\n",
      "* The Mexico Central region has only three regions, suggesting a smaller market share compared to other regions.\n",
      "* The Canada West region is represented by five regions, indicating a significant presence in Vancouver.\n",
      "\n",
      "**Recommendations**\n",
      "\n",
      "1. **Focus on North West and South West regions**: Given the strong presence of these regions, it may be beneficial to focus sales efforts and resources on these areas.\n",
      "2. **Expand into Mexico Central region**: Considering the smaller market share, investing time and resources into this region could lead to new opportunities.\n",
      "3. **Maintain a presence in Canada West region**: The five regions in Vancouver suggest a significant market opportunity; maintaining a presence here could be beneficial for long-term growth.\n",
      "\n",
      "These insights can help inform sales strategies, resource allocation, and business decisions to maximize revenue and growth in these regions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "📌 **الإجابة النهائية:**\n",
      "\n",
      "**Analysis of the Dataset**\n",
      "\n",
      "The dataset consists of 114 regions with their corresponding sales districts and regions. Here are some key observations:\n",
      "\n",
      "1. **North West Region**: The North West region has 23 regions, all located in Washington state. This suggests a strong focus on this region.\n",
      "2. **South West Region**: The South West region has 21 regions, all located in California. This also indicates a significant presence in this region.\n",
      "3. **Mexico Central Region**: There are three regions in the Mexico Central region, indicating a smaller but still notable presence in this area.\n",
      "4. **Canada West Region**: Five regions are located in the Canada West region, primarily in Vancouver.\n",
      "\n",
      "**Key Insights**\n",
      "\n",
      "* The North West and South West regions have a strong focus on California and Washington state, respectively.\n",
      "* The Mexico Central region has only three regions, suggesting a smaller market share compared to other regions.\n",
      "* The Canada West region is represented by five regions, indicating a significant presence in Vancouver.\n",
      "\n",
      "**Recommendations**\n",
      "\n",
      "1. **Focus on North West and South West regions**: Given the strong presence of these regions, it may be beneficial to focus sales efforts and resources on these areas.\n",
      "2. **Expand into Mexico Central region**: Considering the smaller market share, investing time and resources into this region could lead to new opportunities.\n",
      "3. **Maintain a presence in Canada West region**: The five regions in Vancouver suggest a significant market opportunity; maintaining a presence here could be beneficial for long-term growth.\n",
      "\n",
      "These insights can help inform sales strategies, resource allocation, and business decisions to maximize revenue and growth in these regions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "from Main import analysis_data\n",
    "# ✅ **1. تحميل ملف CSV وتحويله إلى نصوص**\n",
    "file_path = \"Regions.csv\"  # ضع اسم ملفك هنا\n",
    "df = pd.read_csv(file_path)  # قراءة ملف CSV\n",
    "\n",
    "# ✅ **2. تحويل كل صف إلى نص**\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    text = \" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns])  # تحويل كل صف إلى نص\n",
    "    documents.append(text)\n",
    "\n",
    "# ✅ **3. تقسيم البيانات إلى أجزاء صغيرة**\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_text(\"\\n\".join(documents))\n",
    "\n",
    "# ✅ **4. تحويل البيانات إلى Embeddings وتخزينها**\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "vector_db = FAISS.from_texts(split_docs, embedding_model)\n",
    "\n",
    "# ✅ **5. إنشاء RAG Retriever**\n",
    "retriever = vector_db.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"data_analysis\",\n",
    "    \"Analyze CSV data and provide insights.\"\n",
    ")\n",
    "\n",
    "# ✅ **6. تجهيز Agent**\n",
    "tools = [retriever_tool]\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# ✅ **7. تنفيذ الاستعلام باستخدام Agent**\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": \"Analyze the dataset \"})\n",
    "\n",
    "response_text = response[\"output\"]  # استخراج النص الصحيح من القاموس\n",
    "print(\"\\n📌 **الإجابة النهائية:**\\n\")\n",
    "for line in response_text.split(\"\\n\"):\n",
    "    print(line.strip()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools from langchain_community.tools import WikipediaQueryRun  ## execute a query and retrieve the results\n",
    "from langchain_community.utilities import WikipediaAPIWrapper   ## set up the connection or configuration for interacting with Wikipedia.import WikipediaQueryRun  ## execute a query and retrieve the results\n",
    "from langchain_community.utilities import WikipediaAPIWrapper   ## set up the connection or configuration for interacting with Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=500)  #يجيب اول تلت نتائج  وميزيدش عن 500 حرف\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_20092\\2031108920.py:22: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BD175D010>, search_kwargs={})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load\n",
    "#chunk \n",
    "#embedding\n",
    "#vector db\n",
    "#retriever \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents from the web\n",
    "load_web = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs_web = load_web.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "split_web = text_splitter.split_documents(docs_web)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(split_web, embedding_model)\n",
    "\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever_web = vector_db.as_retriever()\n",
    "\n",
    "retriever_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a retriever tool for the vector database\n",
    "from langchain.tools.retriever import create_retriever_tool         ## used to search for information in a vector database or other data source\n",
    "\n",
    "# Create the retriever tool with the vector database and a description\n",
    "retriever_web_tool = create_retriever_tool(retriever_web,\n",
    "                    \"langsmith_search\",  ## used when integrating the tool into an agent or pipeline \n",
    "                    \"Search for information about LangSmith.For any questions about LangSmith, you must use this tool!\")\n",
    "\n",
    "retriever_web_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Arxiv Tool and Utilities for Creating and Reading Applications\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun   ##  query the Arxiv database and retrieve relevant information\n",
    "\n",
    "# Set up the connection to Arxiv API\n",
    "arxiv_api = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "\n",
    "# Create an ArxivQueryRun tool\n",
    "arxiv_query_run = ArxivQueryRun(arxiv_api=arxiv_api)\n",
    "\n",
    "arxiv_query_run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\oa196\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=500)),\n",
       " Tool(name='langsmith_search', description='Search for information about LangSmith.For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000028BBA2BB420>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BD175D010>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000028BBA5A6FC0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BD175D010>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n')),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the tools list\n",
    "tools = [wiki, retriever_web_tool, arxiv_query_run]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_20092\\1943383421.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:3b\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3.2:3b')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Ollama LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## agent\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000028BB9943A60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000028BB9943A60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=Ollama(model='llama3.2:3b'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith.For any questions about LangSmith, you must use this tool!', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\oa196\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=500)), Tool(name='langsmith_search', description='Search for information about LangSmith.For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000028BBA2BB420>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BD175D010>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000028BBA5A6FC0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BD175D010>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n')), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This output parser only works on ChatGeneration output",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about LangSmith\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      2\u001b[0m response\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n",
      "\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n",
      "\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n",
      "\u001b[0;32m    163\u001b[0m     )\n",
      "\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n",
      "\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n",
      "\u001b[0;32m    167\u001b[0m     )\n",
      "\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n",
      "\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n",
      "\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n",
      "\u001b[1;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n",
      "\u001b[0;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n",
      "\u001b[0;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n",
      "\u001b[0;32m   1634\u001b[0m         )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\agent.py:1332\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n",
      "\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n",
      "\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n",
      "\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n",
      "\u001b[0;32m   1330\u001b[0m         [\n",
      "\u001b[0;32m   1331\u001b[0m             a\n",
      "\u001b[1;32m-> 1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n",
      "\u001b[0;32m   1333\u001b[0m                 name_to_tool_map,\n",
      "\u001b[0;32m   1334\u001b[0m                 color_mapping,\n",
      "\u001b[0;32m   1335\u001b[0m                 inputs,\n",
      "\u001b[0;32m   1336\u001b[0m                 intermediate_steps,\n",
      "\u001b[0;32m   1337\u001b[0m                 run_manager,\n",
      "\u001b[0;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m   1339\u001b[0m         ]\n",
      "\u001b[0;32m   1340\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n",
      "\u001b[0;32m   1355\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n",
      "\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n",
      "\u001b[1;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\agent.py:581\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    573\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n",
      "\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n",
      "\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    579\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n",
      "\u001b[0;32m    580\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n",
      "\u001b[1;32m--> 581\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[0;32m    583\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3407\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   3401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n",
      "\u001b[0;32m   3402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m   3403\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n",
      "\u001b[0;32m   3404\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m   3405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n",
      "\u001b[0;32m   3406\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n",
      "\u001b[1;32m-> 3407\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3394\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   3388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n",
      "\u001b[0;32m   3389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m   3390\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n",
      "\u001b[0;32m   3391\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m   3392\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n",
      "\u001b[0;32m   3393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n",
      "\u001b[1;32m-> 3394\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n",
      "\u001b[0;32m   3395\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n",
      "\u001b[0;32m   3396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n",
      "\u001b[0;32m   3397\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n",
      "\u001b[0;32m   3398\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m   3399\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2197\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;32m-> 2197\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;32m   2198\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[0;32m   2199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3357\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   3354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m   3355\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n",
      "\u001b[1;32m-> 3357\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1428\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n",
      "\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:998\u001b[0m, in \u001b[0;36mRunnable.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n",
      "\u001b[0;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    982\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n",
      "\u001b[0;32m    983\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    984\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n",
      "\u001b[0;32m    985\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n",
      "\u001b[0;32m    986\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    Default implementation of stream, which calls invoke.\u001b[39;00m\n",
      "\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    996\u001b[0m \u001b[38;5;124;03m        The output of the Runnable.\u001b[39;00m\n",
      "\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 998\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:202\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n",
      "\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n",
      "\u001b[0;32m    195\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    199\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m    200\u001b[0m     )\n",
      "\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n",
      "\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n",
      "\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n",
      "\u001b[0;32m   1926\u001b[0m         Output,\n",
      "\u001b[1;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[0;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[0;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[0;32m   1935\u001b[0m     )\n",
      "\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n",
      "\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n",
      "\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:203\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n",
      "\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n",
      "\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n",
      "\u001b[0;32m    195\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    199\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m    200\u001b[0m     )\n",
      "\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n",
      "\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[0;32m    204\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n",
      "\u001b[0;32m    205\u001b[0m         config,\n",
      "\u001b[0;32m    206\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m    207\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\output_parsers\\openai_tools.py:60\u001b[0m, in \u001b[0;36mOpenAIToolsAgentOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\n",
      "\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[List[AgentAction], AgentFinish]:\n",
      "\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result[\u001b[38;5;241m0\u001b[39m], ChatGeneration):\n",
      "\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis output parser only works on ChatGeneration output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     61\u001b[0m     message \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_ai_message_to_openai_tool_action(message)\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: This output parser only works on ChatGeneration output"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Tell me about LangSmith\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangsmith is an American poet, songwriter, and satirist. He was born in 1961 in Philadelphia, Pennsylvania.\n",
      "\n",
      "Langsmith is best known for his humorous and insightful poetry, which often focuses on politics, social issues, and pop culture. His work has been widely published and has appeared in various media outlets, including The New Yorker, The Nation, and NPR.\n",
      "\n",
      "Some of Langsmith's notable works include:\n",
      "\n",
      "1. \"The Poet Laureate of America's Grown-Up Children\": This is a satirical poem that pokes fun at the role of poetry in American culture.\n",
      "2. \"America's Favorite Pastime: Football\": This poem explores the cultural significance of football in America and the ways in which it shapes our society.\n",
      "3. \"The Great Unraveling\": This poem examines the complexities of politics and social change, using humor to highlight the absurdities of modern life.\n",
      "\n",
      "Langsmith has also written several books of poetry, including \" Langsmith's America: A Poet's Guide to Modern Life\" and \"The Limerick Poems of Langsmith\". His work has been praised for its wit, intelligence, and accessibility.\n",
      "\n",
      "Overall, Langsmith is a talented poet and satirist who uses humor and insight to comment on the world around us.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me about Langsmith', 'output': 'Langsmith is an American poet, songwriter, and satirist. He was born in 1961 in Philadelphia, Pennsylvania.\\n\\nLangsmith is best known for his humorous and insightful poetry, which often focuses on politics, social issues, and pop culture. His work has been widely published and has appeared in various media outlets, including The New Yorker, The Nation, and NPR.\\n\\nSome of Langsmith\\'s notable works include:\\n\\n1. \"The Poet Laureate of America\\'s Grown-Up Children\": This is a satirical poem that pokes fun at the role of poetry in American culture.\\n2. \"America\\'s Favorite Pastime: Football\": This poem explores the cultural significance of football in America and the ways in which it shapes our society.\\n3. \"The Great Unraveling\": This poem examines the complexities of politics and social change, using humor to highlight the absurdities of modern life.\\n\\nLangsmith has also written several books of poetry, including \" Langsmith\\'s America: A Poet\\'s Guide to Modern Life\" and \"The Limerick Poems of Langsmith\". His work has been praised for its wit, intelligence, and accessibility.\\n\\nOverall, Langsmith is a talented poet and satirist who uses humor and insight to comment on the world around us.'}\n"
     ]
    }
   ],
   "source": [
    "# used ollama\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Use local embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models import ChatOllama  # Use Ollama instead of OpenAI\n",
    "\n",
    "# Set up Wikipedia search\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# Load and process LangSmith documentation\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "split_web = text_splitter.split_documents(docs_web)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(split_web, embedding_model)\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever_web = vector_db.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever_web,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\"\n",
    ")\n",
    "\n",
    "# Set up Arxiv search\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "# Define tools\n",
    "tools = [wiki, arxiv, retriever_tool]\n",
    "\n",
    "# Replace OpenAI with Ollama\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")  # Change to your desired Ollama model\n",
    "\n",
    "# Get the prompt template\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "response = agent_executor.invoke({\"input\": \"Tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_10180\\3488324316.py:25: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n",
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\oa196\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\oa196\\AppData\\Local\\Temp\\ipykernel_10180\\3488324316.py:40: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2:3b\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m tools \u001b[38;5;241m=\u001b[39m [retriever_tool]\n\u001b[0;32m     44\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyze the CSV file based on PDF guidelines.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_openai_tools_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oa196\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain\\agents\\openai_tools\\base.py:88\u001b[0m, in \u001b[0;36mcreate_openai_tools_agent\u001b[1;34m(llm, tools, prompt, strict)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_openai_tools_agent\u001b[39m(\n\u001b[0;32m     16\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m     17\u001b[0m     tools: Sequence[BaseTool],\n\u001b[0;32m     18\u001b[0m     prompt: ChatPromptTemplate,\n\u001b[0;32m     19\u001b[0m     strict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable:\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an agent that uses OpenAI tools.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     missing_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39mdifference(\n\u001b[1;32m---> 88\u001b[0m         \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mpartial_variables)\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'input_variables'"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import pandas as pd\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "# Load and process PDF\n",
    "pdf_path = \"storying.pdf\"  # Change to your PDF file\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Split PDF text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "pdf_chunks = text_splitter.create_documents([pdf_text])\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Store PDF knowledge in a vector database\n",
    "vector_db = FAISS.from_documents(pdf_chunks, embedding_model)\n",
    "retriever = vector_db.as_retriever()\n",
    "retriever_tool = create_retriever_tool(retriever, \"pdf_guidelines\", \"Retrieve insights from the PDF guidelines.\")\n",
    "\n",
    "# Load CSV\n",
    "def load_csv(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "csv_path = \"Regions.csv\"  # Change to your CSV file\n",
    "df = load_csv(csv_path)\n",
    "\n",
    "# Set up AI model\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Define agent\n",
    "tools = [retriever_tool]\n",
    "prompt = \"Analyze the CSV file based on PDF guidelines.\"\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run analysis\n",
    "response = agent_executor.invoke({\"input\": \"Analyze the provided CSV file using the PDF instructions.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
