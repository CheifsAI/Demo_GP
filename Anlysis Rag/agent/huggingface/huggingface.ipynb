{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_13656\\3629795972.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n"
     ]
    }
   ],
   "source": [
    "# Load PDFs from directory\n",
    "loader = PyPDFDirectoryLoader(\"./data\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_split = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding using Huggingface\n",
    "# generating text embeddings\n",
    "huggingface_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", # generating high-quality embeddings for English text\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(huggingface_embedding.embed_query(text_split[0].page_content))\n",
    "np.array(huggingface_embedding.embed_query(text_split[0].page_content)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector database\n",
    "db1 = FAISS.from_documents(text_split[:120], huggingface_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 U.S. Census Bureau\\nWHAT IS HEALTH INSURANCE COVERAGE?\\nThis brief presents state-level estimates of health insurance coverage \\nusing data from the American Community Survey (ACS). The  \\nU.S. Census Bureau conducts the ACS throughout the year; the \\nsurvey asks respondents to report their coverage at the time of \\ninterview. The resulting measure of health insurance coverage, \\ntherefore, reflects an annual average of current comprehensive \\nhealth insurance coverage status.* This uninsured rate measures a \\ndifferent concept than the measure based on the Current Population \\nSurvey Annual Social and Economic Supplement (CPS ASEC). \\nFor reporting purposes, the ACS broadly classifies health insurance \\ncoverage as private insurance or public insurance. The ACS defines \\nprivate health insurance as a plan provided through an employer \\nor a union, coverage purchased directly by an individual from an \\ninsurance company or through an exchange (such as healthcare.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the vector database\n",
    "query = \"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
    "\n",
    "search = db1.similarity_search(query)\n",
    "search[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027C69C1F140>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "retriever = db1.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'mistralai/Mi...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the HuggingFaceHub model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m hf\u001b[38;5;241m=\u001b[39m\u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the health insurance coverage?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m hf\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'mistralai/Mi...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Initialize the HuggingFaceHub model\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    ")\n",
    "\n",
    "query=\"What is the health insurance coverage?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"DIFFERENCES IN THE UNINSURED RATE BY STATE IN 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "comparison of ACS and CPS ASEC measures \n",
      "of health insurance coverage, refer to < www.\n",
      "census.gov/topics/health/health-insurance/\n",
      "guidance.html >.\n",
      "9 Respondents may have more than one \n",
      "health insurance coverage type at the time \n",
      "of interview. As a result, adding the total \n",
      "number of people with private coverage and \n",
      "the total number with public coverage will \n",
      "sum to more than the total number with any \n",
      "coverage.• From 2021 to 2022, nine states \n",
      "reported increases in private \n",
      "coverage, while seven reported \n",
      "decreases (Appendix Table B-2). \n",
      "DIFFERENCES IN THE \n",
      "UNINSURED RATE BY STATE \n",
      "IN 2022\n",
      "In 2022, uninsured rates at the \n",
      "time of interview ranged across \n",
      "states from a low of 2.4 percent \n",
      "in Massachusetts to a high of 16.6 \n",
      "percent in Texas, compared to the \n",
      "national rate of 8.0 percent.10 Ten \n",
      "of the 15 states with uninsured \n",
      "10 The uninsured rates in the District \n",
      "of Columbia and Massachusetts were not \n",
      "statistically different.rates above the national aver -\n",
      "\n",
      "percent (Appendix Table B-5). \n",
      "Medicaid coverage accounted \n",
      "for a portion of that difference. \n",
      "Medicaid coverage was 22.7 per -\n",
      "cent in the group of states that \n",
      "expanded Medicaid eligibility and \n",
      "18.0 percent in the group of nonex -\n",
      "pansion states.\n",
      "CHANGES IN THE UNINSURED \n",
      "RATE BY STATE FROM 2021 \n",
      "TO 2022\n",
      "From 2021 to 2022, uninsured rates \n",
      "decreased across 27 states, while \n",
      "only Maine had an increase. The \n",
      "uninsured rate in Maine increased \n",
      "from 5.7 percent to 6.6 percent, \n",
      "although it remained below the \n",
      "national average. Maine’s uninsured \n",
      "rate was still below 8.0 percent, \n",
      "21 Douglas Conway and Breauna Branch, \n",
      "“Health Insurance Coverage Status and Type \n",
      "by Geography: 2019 and 2021,” 2022, < www.\n",
      "census.gov/content/dam/Census/library/\n",
      "publications/2022/acs/acsbr-013.pdf >.\n",
      "\n",
      "library/publications/2022/acs/acsbr-013.pdf >.\n",
      "39 In 2022, the private coverage rates were \n",
      "not statistically different in North Dakota and \n",
      "Utah.Figure /five.tab/period.tab\n",
      "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
      "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
      "(Civilian, noninstitutionalized population) /uni00A0\n",
      "* Denotes a statistically signiﬁcant change between 2021 and 2022 at the 90 percent conﬁdence level.\n",
      "Note: For information on conﬁdentiality protection, sampling error, nonsampling error, and deﬁnitions in the American Community\n",
      "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
      "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
      "San Francisco-Oakland-Berkeley/comma.tab CA\n",
      "*Detroit-Warren-Dearborn/comma.tab MI\n",
      "Question:DIFFERENCES IN THE UNINSURED RATE BY STATE IN 2022\n",
      "\n",
      "Helpful Answers:\n",
      "- The uninsured rate in Massachusetts was 2.4% in 2022.\n",
      "- The uninsured rate in Texas was 16.6% in 2022.\n",
      "- The national uninsured rate was 8.0% in 2022.\n",
      "- Ten states had uninsured rates above the national average of 8.0% in 2022.\n",
      "- Medicaid coverage was higher in states that expanded Medicaid eligibility (22.7%) compared to nonexpansion states (18.0%).\n",
      "- Maine was the only state with an increase in the uninsured rate from 2021 to 2022, increasing from 5.7% to 6.6%.\n",
      "- The private coverage rates in North Dakota and Utah were not statistically different in 2022.\n",
      "- The uninsured rates for the most populous metropolitan areas in 2022 were: Boston-Cambridge-Newton, MA-NH (3.5%), San Francisco-Oakland-Berkeley, CA (4.6%), and Detroit-Warren-Dearborn, MI (13.2%).\n"
     ]
    }
   ],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "region_id: 19 | sales_district: Seattle | sales_region: North West\n",
      "\n",
      "region_id: 23 | sales_district: Salem | sales_region: North West\n",
      "\n",
      "region_id: 22 | sales_district: Portland | sales_region: North West\n",
      "Question: \n",
      "        You are a data analyst. You are provided with a dataset about <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      "\n",
      "        Here is the dataset structure:\n",
      "        <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      "\n",
      "\n",
      "        Please analyze the data and provide insights about:\n",
      "        1. Key trends and patterns in the <class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   region_id       109 non-null    int64 \n",
      " 1   sales_district  109 non-null    object\n",
      " 2   sales_region    109 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      ".\n",
      "        2. Any anomalies or outliers in the data.\n",
      "        3. Recommendations or actionable insights based on the analyzed data.\n",
      "         \"\"\"\n",
      "\n",
      "        # 1. Key trends and patterns in the data\n",
      "        # - The dataset contains 109 entries with 3 columns: region_id, sales_district, and sales_region.\n",
      "        # - The region_id column is of integer data type, while sales_district and sales_region are of object data type.\n",
      "        # - There are no missing values in the dataset.\n",
      "\n",
      "        # 2. Anomalies or outliers in the data\n",
      "        # - Since the dataset is small and does not contain any numerical data, it is difficult to identify anomalies or outliers.\n",
      "        # - However, if there are any duplicate entries or inconsistent data in the sales_district and sales_region columns, they should be investigated.\n",
      "\n",
      "        # 3. Recommendations or actionable insights based on the analyzed data\n",
      "        # - To gain more insights from the data, additional data columns could be added, such as sales amount, number of customers, or product categories.\n",
      "        # - Analyzing the sales_district and sales_region columns can provide insights into the distribution of sales across different regions and districts.\n",
      "        # - Further analysis can be conducted to identify the top-performing regions or districts, as well as any trends or patterns in sales performance.\n",
      "\n",
      "Helpful Answers:\n",
      "1. Key trends and patterns in the data:\n",
      "   - All entries in the dataset belong to the North West sales region.\n",
      "   - The sales_district column has three unique values: Seattle, Salem, and Portland.\n",
      "   - The region_id column has three unique values: 19, 22, and 23.\n",
      "\n",
      "2. Anomalies or outliers in the data:\n",
      "   - There are no apparent anomalies or outliers in the given dataset, as all the data seems to be consistent and valid.\n",
      "\n",
      "3. Recommendations or actionable insights based on the analyzed data:\n",
      "   - To gain more insights, consider adding more columns to the dataset, such as sales amount, number of customers, or product categories.\n",
      "   - Analyze the sales performance for each sales_district within the North West sales_region to identify top-performing districts.\n",
      "   - Investigate any trends or patterns in sales performance over time, if historical data is available.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DataAnalyzer import DataAnalyzer\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face API token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Regions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create LangChain Document objects\n",
    "documents = [\n",
    "    Document(page_content=\" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns]))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "# Initialize the vector database\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)\n",
    "\n",
    "# Embedding using Huggingface\n",
    "huggingface_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # High-quality embeddings for English text\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Create the FAISS vector store\n",
    "db1 = FAISS.from_documents(text_split[:120], huggingface_embedding)\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = db1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize the HuggingFaceHub model\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Initialize the DataAnalyzer\n",
    "analyzer = DataAnalyzer(df, llm=hf)  # Use hf instead of retrievalQA\n",
    "\n",
    "# Define the query\n",
    "query = analyzer.analysis_data()\n",
    "\n",
    "# Call the QA chain with our query\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context.\n",
      "\n",
      "region_id: 105 | sales_district: Victoria | sales_region: Canada West\n",
      "\n",
      "region_id: 42 | sales_district: San Francisco | sales_region: Central West\n",
      "\n",
      "region_id: 31 | sales_district: San Francisco | sales_region: Central West\n",
      "Question: Analyze this dataset based on statistical trends and patterns.\n",
      "\n",
      "Helpful Answers:\n",
      "- The sales_district \"San Francisco\" appears twice, both times in the sales_region \"Central West\".\n",
      "- The sales_region \"Canada West\" appears only once, associated with the sales_district \"Victoria\".\n",
      "- The sales_region \"Central West\" appears twice, associated with the sales_district \"San Francisco\" both times.\n",
      "- There are no duplicate region_id values in the dataset.\n",
      "- The dataset contains 3 unique region_id values, 2 unique sales_district values, and 2 unique sales_region values.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# تأكد من وجود التوكن الخاص بك على Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "\n",
    "# تحميل بيانات CSV\n",
    "file_path = \"Regions.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# إنشاء مستندات من البيانات\n",
    "documents = [\n",
    "    Document(page_content=\" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns]))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# تقسيم النصوص إلى أجزاء صغيرة\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# تهيئة نموذج التضمين\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# إنشاء قاعدة بيانات المتجهات\n",
    "vector_db = FAISS.from_documents(text_split, embedding_model)\n",
    "\n",
    "# إعداد مسترجع المعلومات\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# إعداد نموذج اللغة من Hugging Face\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    ")\n",
    "\n",
    "# إعداد قالب التوجيه\n",
    "prompt_template = \"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# إعداد سلسلة استرجاع الأسئلة والإجابات\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# تحديد الاستعلام لتحليل البيانات\n",
    "query = \"Analyze this dataset based on statistical trends and patterns.\"\n",
    "\n",
    "# تنفيذ التحليل\n",
    "result = retrievalQA.invoke(query)\n",
    "\n",
    "# طباعة النتيجة\n",
    "print(\"Analysis Result:\")\n",
    "print(result.get('result', 'No result found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Loading existing FAISS index...\n",
      "\n",
      "🔍 Final Analysis Result:\n",
      "Based on the context provided, here are some potential actionable insights and recommendations that could be derived from the analysis:\n",
      "\n",
      "1. Investigate the reasons behind the higher average sales amounts in the sales district: Is there a specific strategy or approach being used in the districts that is leading to higher sales? Are there any differences in the customer base or market conditions that could explain the difference?\n",
      "2. Analyze the segmented data to identify patterns and trends: By breaking down the data into different segments based on regions, districts, or other factors, it may be possible to identify unique opportunities or challenges for each segment. For example, are there any specific regions where sales are underperforming, and if so, what could be the reasons?\n",
      "3. Develop targeted marketing campaigns: Based on the analysis, it may be worth identifying which regions or districts are underperforming and developing targeted marketing campaigns to improve sales in those areas. This could involve tailoring the messaging or offerings to better resonate with the local customer base.\n",
      "4. Review and optimize the sales process: If there are significant differences in sales amounts across regions or districts, it may be worth reviewing the sales process to identify any inefficiencies or areas for improvement. This could involve streamlining the sales funnel, providing better training or support for sales teams, or adjusting pricing strategies.\n",
      "5. Consider the potential impact of external factors: The analysis may also reveal patterns and trends that are influenced by external factors such as economic conditions, competition, or regulatory changes. It may be worth considering how these factors could be impacting sales performance and what strategies could be implemented to mitigate any negative effects.\n",
      "\n",
      "Overall, the findings suggest that there are opportunities to gain insights into the sales process and identify areas for improvement. By analyzing the data further and taking appropriate actions, it may be possible to increase sales performance and improve overall business outcomes.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate  # ✅ Fixed missing import\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # Importing the class from another file\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1️⃣ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2️⃣ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\n🔄 Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True  # ✅ Fix applied\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n🛠️ Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "    \n",
    "    # Define custom prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following piece of context to answer the question asked.\n",
    "    Please try to provide the answer only based on the context.\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}  # ✅ Added prompt to chain\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3️⃣ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# 🚀 Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"🚨 Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # Create DataAnalyzer instance\n",
    "    analyzer = DataAnalyzer(df, llm=llm)\n",
    "    \n",
    "    # Perform data analysis\n",
    "    query = analyzer.analysis_data()\n",
    "    \n",
    "    # Use RetrievalQA to answer the query\n",
    "    result = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "    # Display the final result\n",
    "    print(\"\\n🔍 Final Analysis Result:\")\n",
    "    print(result['result'])\n",
    "\n",
    "    # Display source documents\n",
    "    print(\"\\n📚 Source Documents:\")\n",
    "    for doc in result['source_documents']:\n",
    "        print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
