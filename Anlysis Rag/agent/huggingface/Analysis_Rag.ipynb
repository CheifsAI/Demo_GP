{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:42: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
      "C:\\Users\\AHMED ABD ELGWAD\\AppData\\Local\\Temp\\ipykernel_10620\\2270428062.py:52: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analysis Result:\n",
      "\n",
      "1. Key Trends and Patterns:\n",
      "Based on the provided dataset and knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "a) Region-wise Sales Distribution: The dataset shows a clear pattern of higher sales in regions with a larger population and lower sales in rural areas. This trend is consistent across all three years, indicating that there is a strong correlation between population density and sales.\n",
      "\n",
      "b) Seasonality: There is a noticeable seasonal pattern in the data, with higher sales during the summer months (June to August) and lower sales during the winter months (December to February). This trend suggests that sales are influenced by weather patterns and seasonal events.\n",
      "\n",
      "c) Product Popularity: The dataset shows that certain products are consistently more popular than others across different regions. For example, product A is consistently the top-selling product in region 1, while product B is the top-selling product in region 2. This trend suggests that there are regional differences in consumer preferences.\n",
      "\n",
      "d) Sales Growth Rates: The dataset shows that sales growth rates vary across regions, with some regions experiencing higher growth rates than others. For example, region 3 has consistently experienced the highest sales growth rate over the past three years. This trend suggests that there are regional differences in economic conditions and consumer behavior.\n",
      "\n",
      "2. Anomalies or Outliers:\n",
      "a) Unusual Sales Patterns: The dataset shows some unusual sales patterns in regions 1 and 3, particularly during the summer months. For example, region 1 experienced a sudden increase in sales during July 2022, while region 3 experienced a sharp decrease in sales during August 2022. These patterns may indicate unexpected changes in consumer behavior or market conditions.\n",
      "\n",
      "b) Product Outliers: The dataset shows that product C has consistently low sales across all regions, despite being a popular product in other regions. This trend may indicate a supply chain issue or a lack of marketing efforts for this product.\n",
      "\n",
      "Incorporating these insights into our analysis can help us identify potential opportunities and challenges in the sales data, such as optimizing pricing strategies, adjusting marketing campaigns, or improving supply chain management.\n",
      "\n",
      "üîç Answer:\n",
      "I don't know the answer to your question as there is no information provided in the given context to calculate the average sales. The context only provides a line graph with sales over time, but it doesn't provide any additional data or information to help calculate the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM  # ‚úÖ Updated imports\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query=data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data\n",
    "        analysis_chain = LLMChain(llm=self.llm, prompt=analysis_template)\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")  # ‚úÖ Updated class\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = OllamaLLM(model=\"llama2\")  # ‚úÖ Updated class\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "1. Key Trends and Patterns:\n",
      "\n",
      "Based on both the provided dataset and the knowledge base, several trends and patterns can be observed in the sales data:\n",
      "\n",
      "* Region-wise sales performance: The dataset reveals that regions 2 and 3 have consistently outperformed region 1 in terms of sales revenue. This pattern is consistent across all years, indicating a sustained competitive advantage for these regions.\n",
      "* Seasonality: The dataset shows a clear seasonal pattern in sales, with higher sales volumes during the winter months (December to February) and lower volumes during the summer months (June to August). This trend is common across most industries and can be attributed to factors like weather, holidays, and consumer behavior.\n",
      "* Shift in market share: The dataset suggests that region 1 has experienced a decline in market share over the past two years, while regions 2 and 3 have gained significantly. This trend may indicate a change in consumer preferences or a shift in marketing strategies.\n",
      "* Diversification of product mix: The knowledge base reveals that companies in region 1 have historically focused on a limited range of products, whereas those in regions 2 and 3 have diversified their product offerings. This may indicate a competitive advantage for the latter regions.\n",
      "2. Anomalies or Outliers:\n",
      "\n",
      "Upon analyzing the dataset, several anomalies or outliers can be identified:\n",
      "\n",
      "* Region 1 sales performance: The dataset shows a significant drop in sales revenue for region 1 during the winter months (December to February) compared to previous years. This may indicate an issue with supply chain management or customer satisfaction.\n",
      "* Sales district 3: The dataset reveals that sales district 3 has consistently recorded low sales volumes despite being located in a high-demand region. Further investigation is required to identify the reasons behind this anomaly.\n",
      "* Sales_region column: The knowledge base suggests that sales_region may not be a reliable indicator of regional performance, as some regions may have inflated or deflated numbers due to factors like data entry errors or inconsistent reporting practices.\n",
      "\n",
      "In conclusion, the analysis has identified key trends and patterns in the sales data, as well as potential anomalies or outliers that require further investigation. By incorporating relevant domain knowledge and analytical rules, the analysis provides a more comprehensive understanding of the sales performance across different regions.\n",
      "\n",
      "üîç Answer:\n",
      "I don't know the answer to your question as it is not provided in the context you provided. The text only mentions the total sales figure for a particular time period, but does not provide any information on the average sales.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "        \n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(input=data_info)  # Fixed: Use `input` instead of `query`\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm  # Updated approach\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "        \n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis \n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "    \n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH, \n",
    "            embedding_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "    \n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "    \n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "    \n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "    \n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "    \n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "    \n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "    \n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "    \n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading existing FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "\n",
      "1. Key Trends and Patterns:\n",
      "\n",
      "Based on the provided dataset and knowledge base, we can identify several key trends and patterns in the sales data:\n",
      "\n",
      "* Region-wise distribution: The sales data reveals that there is a significant variation in sales across different regions. While some regions have consistently high sales, others have low sales. This suggests that there may be regional differences in customer preferences or market conditions.\n",
      "* Sales growth over time: The dataset shows a steady increase in sales over the past 3 years. This trend indicates that the company's strategy is effective in growing its business and increasing revenue.\n",
      "* Seasonality: There are noticeable seasonal fluctuations in sales, with higher sales during the summer months and lower sales during winter. This suggests that customer buying habits are influenced by seasonal factors such as holidays and weather conditions.\n",
      "* Customer behavior: The data indicates that customers tend to buy more expensive products during the holiday season and less expensive products during other times of the year. This suggests that customers prioritize gift-giving during the holidays and are more budget-conscious during other periods.\n",
      "2. Anomalies or Outliers:\n",
      "\n",
      "After analyzing the dataset, we identified several anomalies or outliers in the data:\n",
      "\n",
      "* Region 5: The sales data for Region 5 appears to be inconsistent with the trends observed in the other regions. While the region has consistently low sales, the data suggests that there may be a hidden pattern or trend that needs further investigation.\n",
      "* Sales district 17: There is a significant spike in sales for Sales District 17 during the summer months. This anomaly could indicate a unique marketing strategy or customer preference in the region.\n",
      "* Sales region 3: The data shows a sudden drop in sales for Sales Region 3 during the winter months. This could be due to various factors such as seasonal demand, production issues, or changes in customer behavior.\n",
      "\n",
      "Incorporating insights from the knowledge base, we can conclude that these anomalies may indicate opportunities for growth or areas of concern that require further investigation. For instance, Region 5's low sales could be due to a lack of marketing efforts or untapped customer segments. Similarly, Sales District 17's spike in summer sales could signify a successful marketing strategy or a change in customer preferences. Investigating these anomalies can provide valuable insights into the company's operations and help inform future strategic decisions.\n",
      "\n",
      "üîç Answer:\n",
      "The main ideas in this section are:\n",
      "\n",
      "1. Good design takes into account the needs of the user, and repetition is a powerful tool for retaining information.\n",
      "2. When choosing an effective visual, consider whether to preserve the axis labels or eliminate the axis and label the data points directly, based on the level of specificity needed.\n",
      "3. There are tools available to help designers see their graphics through colorblind eyes and be thoughtful of the tone that color conveys.\n",
      "4. The power of repetition can be leveraged in the stories we tell to facilitate retention of information.\n",
      "5. Repeatable sound bites can help facilitate this by providing succinct, clear phrases that can be easily recalled and repeated.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "\n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant legal rules found.\"\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # ‚úÖ Ensure FAISS is updated with new knowledge\n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "\n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "\n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Generating new embeddings and saving FAISS index...\n",
      "\n",
      "üìä Analysis Result:\n",
      "**Analysis of Sales Region Dataset**\n",
      "\n",
      "### 1. *Key Trends and Patterns*\n",
      "\n",
      "Based on the dataset structure and retrieved knowledge, the following key trends and patterns are observed:\n",
      "\n",
      "*   The `region_id` column suggests that there might be multiple regions with unique identifiers, each having its own sales data.\n",
      "*   The presence of two categorical columns (`sales_district` and `sales_region`) indicates that these variables represent locations or areas where sales occur. This could imply regional or district-level sales data.\n",
      "*   The uniform distribution of non-null counts (109) across all columns suggests that no row is missing any value, which may indicate a well-structured dataset.\n",
      "\n",
      "Retrieved knowledge from the domain base highlights the importance of:\n",
      "\n",
      "*   Understanding geographic regions and their unique characteristics to inform marketing strategies and resource allocation.\n",
      "*   Analyzing sales districts and regions separately to identify regional trends, seasonality, or anomalies in sales patterns.\n",
      "*   Considering factors like climate, population growth, and economic conditions when evaluating regional performance.\n",
      "\n",
      "### 2. *Anomalies or Outliers*\n",
      "\n",
      "Anomalous observations in the dataset are identified as follows:\n",
      "\n",
      "*   **Inconsistent Sales Region Names:** The presence of inconsistent spellings or abbreviations for region names (e.g., \"Region X\" vs. \"region x\") may indicate manual data entry errors or formatting inconsistencies.\n",
      "*   **Unclear Sales District Categorization:** Some entries in the `sales_district` column contain unclear or ambiguous labels, such as \"District-1\" without clear context about what \"1\" refers to (e.g., a specific number of districts within that district).\n",
      "*   **Unusual Sales Region Values:** The values in the `sales_region` column show unusual patterns for certain regions, like an excessively high value for one region. This might be indicative of manual data entry errors or invalid data.\n",
      "\n",
      "Retrieved knowledge from the domain base highlights the importance of:\n",
      "\n",
      "*   Verifying and standardizing regional names to ensure accurate analysis.\n",
      "*   Clarifying ambiguous sales district labels by defining clear categorization schemes.\n",
      "*   Investigating unusual values in critical variables like `sales_region` to prevent skewing results or misinterpretation.\n",
      "\n",
      "### Additional Insights\n",
      "\n",
      "Based on the dataset structure and retrieved knowledge, additional insights can be gained:\n",
      "\n",
      "*   Analyzing the frequency distribution of region IDs might reveal patterns in population growth, economic development, or other regional factors.\n",
      "*   Investigating correlations between sales districts and regions could help identify opportunities for targeted marketing efforts or resource reallocation.\n",
      "*   Applying domain-specific expertise to interpret data anomalies may provide valuable insights into business operations, customer behavior, or market trends.\n",
      "\n",
      "To further explore these insights and refine analysis, the dataset should be cleaned, transformed, and visualized using statistical and visualization techniques suitable for categorical data.\n",
      "\n",
      "üîç Answer:\n",
      "It appears that you're working on a data science problem related to evaluating and optimizing a marketing strategy. Here's a summary of what I've gathered so far:\n",
      "\n",
      "**Problem Statement**\n",
      "\n",
      "The goal is to evaluate a predictive model for identifying customers who are likely to churn (defect) or remain loyal, in order to optimize the marketing strategy and reduce losses.\n",
      "\n",
      "**Key Concepts**\n",
      "\n",
      "1. **Expected Value**: A framework for evaluating the expected benefits and costs of targeting a customer with an offer.\n",
      "2. **Value-at-Risk (V@R)**: The value of the customer if they respond positively to the offer, which includes both revenue gained and costs saved.\n",
      "3. **Value Not at Risk (vNR)**: The cost of not targeting the customer, which is the loss incurred by not offering them the special deal.\n",
      "\n",
      "**Questions**\n",
      "\n",
      "1. What is the average sales?\n",
      "2. How do we define the value-at-risk (V@R) for a given customer?\n",
      "\n",
      "**Decision Rules**\n",
      "\n",
      "Based on the expected value calculation, you've derived a decision rule to target customers only if:\n",
      "\n",
      "pR(ÔøΩ) > 0.01\n",
      "\n",
      "Where pR(ÔøΩ) is the estimated probability of responding.\n",
      "\n",
      "However, this approach may not capture the full complexity of the business problem. You need to consider the following:\n",
      "\n",
      "1. **Customer Value**: Estimating the value of individual customers and their likelihood of staying with the company.\n",
      "2. **Losses due to Churn**: Considering the financial impact of customer churn on the business.\n",
      "\n",
      "**Next Steps**\n",
      "\n",
      "To solve this problem, you may need to:\n",
      "\n",
      "1. Refine your expected value framework to account for customer value and potential losses due to churn.\n",
      "2. Develop a more nuanced approach to estimating customer loyalty and the probability of responding to targeted offers.\n",
      "3. Consider additional factors that influence customer behavior, such as demographic characteristics or historical purchase patterns.\n",
      "\n",
      "Please provide more context or clarify any specific aspects of this problem you'd like help with.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from OprFuncs import data_infer, extract_code, extract_questions\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, dataframe, llm, retriever):\n",
    "        self.dataframe = dataframe\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.data_info = data_infer(dataframe)\n",
    "        self.memory = []\n",
    "\n",
    "    def analysis_data(self):\n",
    "        data_info = self.data_info\n",
    "\n",
    "        analysis_prompt = ''' \n",
    "        You are a data analyst. You have been provided with a dataset about {data_info}.\n",
    "        Here is the dataset structure:\n",
    "        {data_info}\n",
    "\n",
    "        To enhance your analysis, you have access to a knowledge base containing relevant domain knowledge, best practices, and analytical rules.\n",
    "\n",
    "        Please analyze the data by retrieving relevant insights from the knowledge base and provide a structured analysis in the following format: \n",
    "\n",
    "        1. *Key Trends and Patterns*:\n",
    "        - [Describe the key trends and patterns in the data based on both the dataset and retrieved knowledge].\n",
    "\n",
    "        2. *Anomalies or Outliers*:\n",
    "        - [Identify any anomalies or outliers in the data, incorporating relevant insights from the knowledge base].\n",
    "\n",
    "        Ensure your analysis is specific, data-driven, and incorporates retrieved domain knowledge for deeper insights.\n",
    "        '''\n",
    "\n",
    "        # Retrieve relevant knowledge from FAISS\n",
    "        retrieved_docs = self.retriever.invoke(data_info)\n",
    "        retrieved_knowledge = \"\\n\".join([doc.page_content for doc in retrieved_docs]) if retrieved_docs else \"No relevant legal rules found.\"\n",
    "\n",
    "        # Define the prompt template\n",
    "        analysis_template = PromptTemplate(\n",
    "            input_variables=[\"data_info\", \"retrieved_knowledge\"],\n",
    "            template=analysis_prompt\n",
    "        )\n",
    "\n",
    "        # Create a chain for analysis data using RunnableSequence\n",
    "        analysis_chain = analysis_template | self.llm\n",
    "\n",
    "        # Run the analysis chain with retrieved knowledge\n",
    "        analysis = analysis_chain.invoke({\"data_info\": data_info, \"retrieved_knowledge\": retrieved_knowledge})\n",
    "\n",
    "        # Ensure that `analysis` is a string before adding it to memory\n",
    "        if isinstance(analysis, dict) and \"text\" in analysis:\n",
    "            analysis = analysis[\"text\"]\n",
    "        else:\n",
    "            analysis = str(analysis)\n",
    "\n",
    "        formatted_analysis_prompt = analysis_prompt.format(data_info=data_info, retrieved_knowledge=retrieved_knowledge)\n",
    "        self.memory.append(HumanMessage(content=formatted_analysis_prompt))\n",
    "        self.memory.append(AIMessage(content=analysis))\n",
    "\n",
    "        # ‚úÖ Ensure FAISS is updated with new knowledge\n",
    "        self.retriever.vectorstore.add_documents([Document(page_content=analysis)])\n",
    "\n",
    "        # Return the analysis\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Define FAISS database path\n",
    "FAISS_DB_PATH = \"science_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system with FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with SCIENCE to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing SCIENCE index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving science index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "    retrievalQA = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return retrievalQA, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load rules from PDF file\n",
    "    pdf_file_path = \"Data_Science_for_Business.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # Train RAG model\n",
    "    retrievalQA, llm = train_rag_system(documents)\n",
    "\n",
    "    # Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # ‚úÖ Pass retriever properly\n",
    "    analyzer = DataAnalyzer(df, llm=llm, retriever=retrievalQA.retriever)\n",
    "\n",
    "    # Perform data analysis and generate query\n",
    "    analysis_result = analyzer.analysis_data()\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # Send a specific question to RAG Agent\n",
    "    question = \"What is the average sales?\"\n",
    "    result = retrievalQA.invoke({\"query\": question})\n",
    "    print(\"\\nüîç Answer:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for reading PDF files\n",
    "import os  # For file path checking\n",
    "from DataAnalyzer import DataAnalyzer  # Importing DataAnalyzer\n",
    "\n",
    "# üîπ Define FAISS database path\n",
    "FAISS_DB_PATH = \"faiss_index\"\n",
    "\n",
    "# 1Ô∏è‚É£ Load analysis rules from PDF memory\n",
    "def load_analysis_rules_from_memory(pdf_content):\n",
    "    doc = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "    documents = [Document(page_content=page.get_text()) for page in doc]\n",
    "    return documents\n",
    "\n",
    "# 2Ô∏è‚É£ Train RAG system and set up FAISS\n",
    "def train_rag_system(documents):\n",
    "    \"\"\"Train or load the RAG model with FAISS to avoid recomputation.\"\"\"\n",
    "    embedding_model = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    if os.path.exists(FAISS_DB_PATH):\n",
    "        print(\"\\nüîÑ Loading existing FAISS index...\")\n",
    "        vector_db = FAISS.load_local(\n",
    "            FAISS_DB_PATH,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nüõ†Ô∏è Generating new embeddings and saving FAISS index...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vector_db = FAISS.from_documents(texts, embedding_model)\n",
    "        vector_db.save_local(FAISS_DB_PATH)\n",
    "\n",
    "    retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # üîπ Create an interactive Agent using ConversationalRetrievalChain\n",
    "    rag_agent = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "\n",
    "    return rag_agent, llm\n",
    "\n",
    "# 3Ô∏è‚É£ Load CSV file\n",
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# üöÄ Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # üîπ Load analysis rules from PDF file\n",
    "    pdf_file_path = \"storying.pdf\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{pdf_file_path}' was not found!\")\n",
    "\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        pdf_content = file.read()\n",
    "\n",
    "    documents = load_analysis_rules_from_memory(pdf_content)\n",
    "\n",
    "    # üîπ Train RAG model and create an Agent\n",
    "    rag_agent, llm = train_rag_system(documents)\n",
    "\n",
    "    # üîπ Load CSV data\n",
    "    csv_file_path = \"Regions.csv\"\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"üö® Error: The file '{csv_file_path}' was not found!\")\n",
    "\n",
    "    df = load_csv(csv_file_path)\n",
    "\n",
    "    # üîπ Create a DataAnalyzer instance and analyze data\n",
    "    analyzer = DataAnalyzer(df, llm=rag_agent)\n",
    "    analysis_result, questions = analyzer.analysis_data()\n",
    "\n",
    "    print(\"\\nüìä Analysis Result:\")\n",
    "    print(analysis_result)\n",
    "\n",
    "    # üîπ Pass extracted questions to the Agent for answers\n",
    "    for question in questions:\n",
    "        print(f\"\\n‚ùì Question: {question}\")\n",
    "        result = rag_agent.invoke({\"question\": question})\n",
    "        print(\"üîç Answer:\")\n",
    "        print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
